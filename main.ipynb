{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19895116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account([('id', 115469357557448283), ('username', 'vuTester'), ('acct', 'vuTester'), ('display_name', 'bendover'), ('discoverable', True), ('group', False), ('locked', False), ('created_at', datetime.datetime(2025, 10, 31, 0, 0, tzinfo=tzutc())), ('following_count', 0), ('followers_count', 0), ('statuses_count', 0), ('note', '<p>bendover</p>'), ('url', 'https://mastodon.social/@vuTester'), ('uri', 'https://mastodon.social/ap/users/115469357557448283'), ('avatar', 'https://files.mastodon.social/accounts/avatars/115/469/357/557/448/283/original/a9e38e3d0d41d484.png'), ('header', 'https://files.mastodon.social/accounts/headers/115/469/357/557/448/283/original/f859c2745152ea94.png'), ('avatar_static', 'https://files.mastodon.social/accounts/avatars/115/469/357/557/448/283/original/a9e38e3d0d41d484.png'), ('header_static', 'https://files.mastodon.social/accounts/headers/115/469/357/557/448/283/original/f859c2745152ea94.png'), ('moved', None), ('suspended', None), ('limited', None), ('bot', False), ('fields', []), ('emojis', []), ('last_status_at', None), ('noindex', False), ('roles', []), ('role', Role([('id', '-99'), ('name', ''), ('permissions', '65536'), ('color', ''), ('highlighted', False)])), ('source', CredentialAccountSource([('privacy', 'public'), ('sensitive', False), ('note', 'bendover'), ('language', None), ('fields', []), ('follow_requests_count', 0), ('indexable', True), ('hide_collections', None), ('discoverable', True), ('attribution_domains', []), ('quote_policy', 'public')])), ('mute_expires_at', None), ('indexable', True), ('hide_collections', None), ('memorial', None)])\n"
     ]
    }
   ],
   "source": [
    "from mastodon import Mastodon\n",
    "\n",
    "API = Mastodon(\n",
    "    client_id='b-1LjDLQvdfa1Kd_uVk4EStyVGWCe_lqipdwKG68LGw',\n",
    "    client_secret='jkWM5f58nofXVQP9-x0sf6yzuB3WfePTiIaHLe97kwM',\n",
    "    access_token='zuG68yri_Fp4RxFro2QzRK4RRAp1bcmFwyRc5Rjb9Sw',\n",
    "    api_base_url='https://mastodon.social'  \n",
    ")\n",
    "\n",
    "me = API.account_verify_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Event: Stable Diffusion public release (2022-08-22) ====================\n",
      "\n",
      "--- Instance: mastodon.social ---\n",
      "Tag: #stable diffusion\n",
      "  Before: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Instances (no API keys needed)\n",
    "# -----------------------------------\n",
    "INSTANCES = [\n",
    "    \"mastodon.social\",\n",
    "    \"sigmoid.social\",\n",
    "    \"hachyderm.io\",\n",
    "    \"fediscience.org\",  \n",
    "    \"universeodon.com\",\n",
    "    \"techhub.social\",\n",
    "    \"mstdn.social\",\n",
    "]\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Events (example: fill out more)\n",
    "#    Use hashtags without the '#'\n",
    "# -----------------------------------\n",
    "\n",
    "events = [\n",
    "    {\n",
    "        \"name\": \"Stable Diffusion public release\",\n",
    "        \"date\": datetime(2022, 8, 22, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"Stability AI releases Stable Diffusion as a publicly available latent diffusion image model, catalyzing the open-source generative image ecosystem.\",\n",
    "        \"hashtags\": [\n",
    "            \"stable diffusion\", \"stablediffusion\", \"stability ai\",\n",
    "            \"dreamstudio\", \"sd 1.4\", \"sd 1.5\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DALL·E 2 research preview\",\n",
    "        \"date\": datetime(2022, 4, 6, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"OpenAI unveils DALL·E 2 as a research preview, demonstrating high-quality text-to-image generation and popularizing the modern wave of AI art models.\",\n",
    "        \"hashtags\": [\n",
    "            \"dall-e 2\", \"dalle 2\", \"dall e 2\", \"openai image\",\n",
    "            \"text-to-image\", \"ai art\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Midjourney v4 beta\",\n",
    "        \"date\": datetime(2022, 11, 10, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"Midjourney releases version 4 in beta, substantially improving image fidelity and becoming a dominant creative GenAI tool in online art communities.\",\n",
    "        \"hashtags\": [\n",
    "            \"midjourney\", \"midjourney v4\", \"mj v4\",\n",
    "            \"aiart\", \"synthography\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ChatGPT launch (GPT-3.5-based)\",\n",
    "        \"date\": datetime(2022, 11, 30, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"OpenAI releases ChatGPT as a research preview, triggering mass public adoption of conversational generative AI and a global AI boom.\",\n",
    "        \"hashtags\": [\n",
    "            \"chatgpt\", \"chat gpt\", \"gpt-3.5\", \"gpt3.5\",\n",
    "            \"openai chatbot\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GPT-4 release\",\n",
    "        \"date\": datetime(2023, 3, 14, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"OpenAI announces GPT-4, a more capable multimodal large language model, used in ChatGPT and integrated into products like Microsoft Copilot.\",\n",
    "        \"hashtags\": [\n",
    "            \"gpt-4\", \"gpt4\", \"gpt 4\",\n",
    "            \"gpt-4 api\", \"gpt-4 release\", \"gpt4 turbo\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama 2 open-source release\",\n",
    "        \"date\": datetime(2023, 7, 18, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"Meta and Microsoft release Llama 2 with a permissive license, greatly accelerating the open-source GenAI ecosystem for text generation.\",\n",
    "        \"hashtags\": [\n",
    "            \"llama 2\", \"llama2\", \"meta llama\",\n",
    "            \"open source llm\", \"meta ai\", \"llm\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"OpenAI Dev Day (GPT-4 Turbo, GPTs, Assistants API)\",\n",
    "        \"date\": datetime(2023, 11, 6, tzinfo=timezone.utc),\n",
    "        \"category\": \"breakthrough\",\n",
    "        \"description\": \"At its first Dev Day, OpenAI announces GPT-4 Turbo, multimodal upgrades, GPTs, and the Assistants API, marking a platform shift for GenAI applications.\",\n",
    "        \"hashtags\": [\n",
    "            \"openai dev day\", \"devday\", \"gpt-4 turbo\",\n",
    "            \"assistants api\", \"custom gpts\", \"openai event\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sam Altman removal and reinstatement at OpenAI\",\n",
    "        \"date\": datetime(2023, 11, 17, tzinfo=timezone.utc),\n",
    "        \"category\": \"controversy\",\n",
    "        \"description\": \"OpenAI's board fires CEO Sam Altman on November 17, 2023, prompting a global backlash and his reinstatement within days, sparking debate about AI governance and safety.\",\n",
    "        \"hashtags\": [\n",
    "            \"sam altman\", \"openai board\", \"altman firing\",\n",
    "            \"openai governance\", \"openai crisis\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EU AI Act provisional agreement\",\n",
    "        \"date\": datetime(2023, 12, 9, tzinfo=timezone.utc),\n",
    "        \"category\": \"regulation\",\n",
    "        \"description\": \"EU institutions reach a provisional deal on the AI Act, introducing risk-based rules and specific obligations for general-purpose and foundation models, including GenAI systems.\",\n",
    "        \"hashtags\": [\n",
    "            \"eu ai act\", \"ai act\", \"european ai law\",\n",
    "            \"foundation models\", \"gpaI\", \"genai regulation\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Getty Images v. Stability AI (copyright dispute over training data)\",\n",
    "        \"date\": datetime(2023, 1, 16, tzinfo=timezone.utc),\n",
    "        \"category\": \"controversy\",\n",
    "        \"description\": \"Getty Images sues Stability AI over alleged unauthorized use of copyrighted images to train Stable Diffusion, becoming a landmark case on GenAI training data and copyright.\",\n",
    "        \"hashtags\": [\n",
    "            \"getty images\", \"stability ai lawsuit\",\n",
    "            \"stable diffusion lawsuit\", \"ai copyright\",\n",
    "            \"training data scraping\"\n",
    "        ],\n",
    "        \"window_days\": 14,\n",
    "    },\n",
    "]\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"AcademicResearchBot/1.0 (contact: your.email@example.com)\"\n",
    "}\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Helpers\n",
    "# -----------------------------------\n",
    "def to_datetime(dt):\n",
    "    if isinstance(dt, datetime):\n",
    "        return dt\n",
    "    if dt.endswith(\"Z\"):\n",
    "        dt = dt[:-1] + \"+00:00\"\n",
    "    return datetime.fromisoformat(dt)\n",
    "\n",
    "def safe_get(url, params=None, max_retries=3, timeout=30):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Request error ({attempt+1}/{max_retries}) on {url}: {e}\")\n",
    "            time.sleep(2 * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def count_tag_in_window(instance, tag, start_dt, end_dt, max_pages=200, limit=40, sleep=1.0, debug=False):\n",
    "    \"\"\"\n",
    "    Count how many statuses on `instance` tagged with `tag`\n",
    "    fall within [start_dt, end_dt).\n",
    "    Uses /api/v1/timelines/tag/{tag}?local=true (no auth).\n",
    "    \"\"\"\n",
    "    base = f\"https://{instance}/api/v1/timelines/tag/{tag}\"\n",
    "    max_id = None\n",
    "    total = 0\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[DEBUG]  Instance={instance}, tag={tag}, window={start_dt} -> {end_dt}\")\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        params = {\"local\": \"true\", \"limit\": limit}\n",
    "        if max_id is not None:\n",
    "            params[\"max_id\"] = max_id\n",
    "\n",
    "        r = safe_get(base, params=params)\n",
    "        if r is None:\n",
    "            if debug:\n",
    "                print(\"  Giving up on this tag/instance for now.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            batch = r.json()\n",
    "        except ValueError:\n",
    "            if debug:\n",
    "                print(\"  Failed to parse JSON, stopping.\")\n",
    "            break\n",
    "\n",
    "        if not batch:\n",
    "            if debug:\n",
    "                print(\"  Empty batch, stopping.\")\n",
    "            break\n",
    "\n",
    "        oldest_in_batch = None\n",
    "        for s in batch:\n",
    "            created_at = to_datetime(s[\"created_at\"])\n",
    "            if oldest_in_batch is None or created_at < oldest_in_batch:\n",
    "                oldest_in_batch = created_at\n",
    "\n",
    "            if start_dt <= created_at < end_dt:\n",
    "                total += 1\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[DEBUG]  Page {page}, got {len(batch)} statuses, oldest={oldest_in_batch}, total={total}\")\n",
    "\n",
    "        max_id = min(int(s[\"id\"]) for s in batch)\n",
    "\n",
    "        # If everything in this batch is already older than the window, we can stop\n",
    "        if oldest_in_batch is not None and oldest_in_batch < start_dt:\n",
    "            if debug:\n",
    "                print(\"[DEBUG]  Oldest status older than start_dt, stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    return total\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Main: loop over events & instances\n",
    "# -----------------------------------\n",
    "results = []\n",
    "\n",
    "for event in events:\n",
    "    name = event[\"name\"]\n",
    "    date = event[\"date\"]\n",
    "    tags = event[\"hashtags\"]\n",
    "    window_days = event[\"window_days\"]\n",
    "\n",
    "    before_start = date - timedelta(days=window_days)\n",
    "    before_end = date\n",
    "    after_start = date\n",
    "    after_end = date + timedelta(days=window_days)\n",
    "\n",
    "    print(f\"\\n==================== Event: {name} ({date.date()}) ====================\")\n",
    "    event_result = {\"name\": name, \"instances\": {}}\n",
    "\n",
    "    for inst in INSTANCES:\n",
    "        print(f\"\\n--- Instance: {inst} ---\")\n",
    "        inst_result = {\"before\": {}, \"after\": {}}\n",
    "\n",
    "        for tag in tags:\n",
    "            print(f\"Tag: #{tag}\")\n",
    "\n",
    "            before_count = count_tag_in_window(\n",
    "                inst, tag, before_start, before_end, debug=False\n",
    "            )\n",
    "            print(f\"  Before: {before_count}\")\n",
    "\n",
    "            after_count = count_tag_in_window(\n",
    "                inst, tag, after_start, after_end, debug=False\n",
    "            )\n",
    "            print(f\"  After:  {after_count}\")\n",
    "\n",
    "            inst_result[\"before\"][tag] = before_count\n",
    "            inst_result[\"after\"][tag] = after_count\n",
    "\n",
    "        event_result[\"instances\"][inst] = inst_result\n",
    "\n",
    "    results.append(event_result)\n",
    "\n",
    "# `results` now holds a nested dict structure with counts per event/instance/tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e06391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching from sigmoid.social...\n",
      "  Request failed: 429 Client Error: Too Many Requests for url: https://sigmoid.social/api/v1/timelines/public?local=true&limit=40&max_id=114784387499940989\n",
      "  Giving up on this instance for now.\n",
      "  Got 4080 statuses\n",
      "\n",
      "Fetching from mastodon.ai...\n",
      "  Connection error (1/3): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "  Connection error (2/3): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "  Connection error (3/3): ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "  Giving up on this instance for now.\n",
      "  Got 0 statuses\n",
      "\n",
      "Fetching from ai.community...\n",
      "  Request failed: 404 Client Error: Not Found for url: https://ai.community/api/v1/timelines/public?local=true&limit=40\n",
      "  Giving up on this instance for now.\n",
      "  Got 0 statuses\n",
      "\n",
      "Fetching from social.ai...\n",
      "  Request failed: 404 Client Error: Not Found for url: https://social.ai/api/v1/timelines/public?local=true&limit=40\n",
      "  Giving up on this instance for now.\n",
      "  Got 0 statuses\n",
      "\n",
      "Fetching from mstdn.art...\n",
      "  Failed to parse JSON, stopping.\n",
      "  Got 0 statuses\n",
      "\n",
      "Fetching from creators.social...\n",
      "  Got 6747 statuses\n",
      "\n",
      "Fetching from glitch.social...\n",
      "  Request failed: 422 Client Error: Unprocessable Content for url: https://glitch.social/api/v1/timelines/public?local=true&limit=40\n",
      "  Giving up on this instance for now.\n",
      "  Got 0 statuses\n",
      "\n",
      "Fetching from fediscience.org...\n",
      "  Request failed: 500 Server Error: Internal Server Error for url: https://fediscience.org/api/v1/timelines/public?local=true&limit=40&max_id=115557140174160224\n",
      "  Giving up on this instance for now.\n",
      "  Got 520 statuses\n",
      "\n",
      "Fetching from datasci.social...\n",
      "  Got 3728 statuses\n",
      "\n",
      "Fetching from scicomm.xyz...\n",
      "  Got 8000 statuses\n",
      "\n",
      "Fetching from mastodon.social...\n",
      "  Got 8000 statuses\n",
      "\n",
      "Fetching from techhub.social...\n",
      "  Got 8000 statuses\n",
      "\n",
      "==================== sigmoid.social ====================\n",
      "Top words:\n",
      "https                1166\n",
      "midjourney           1014\n",
      "aiart                1013\n",
      "exist                845\n",
      "com                  841\n",
      "org                  380\n",
      "2025                 369\n",
      "new                  324\n",
      "now                  238\n",
      "data                 217\n",
      "models               200\n",
      "bsky                 188\n",
      "post                 187\n",
      "model                187\n",
      "synthography         176\n",
      "warning              174\n",
      "reading              172\n",
      "blog                 169\n",
      "people               163\n",
      "work                 162\n",
      "llm                  162\n",
      "app                  160\n",
      "research             157\n",
      "here                 154\n",
      "social               152\n",
      "time                 144\n",
      "paper                143\n",
      "github               140\n",
      "been                 130\n",
      "llms                 129\n",
      "\n",
      "Top hashtags:\n",
      "#aiart               1013\n",
      "#midjourney          1013\n",
      "#ai                  277\n",
      "#synthography        176\n",
      "#reading             152\n",
      "#mlsec               88\n",
      "#ml                  61\n",
      "#machinelearning     61\n",
      "#neuroscience        59\n",
      "#llm                 45\n",
      "#knowledgegraphs     42\n",
      "#compneuro           41\n",
      "#artificialintelligence 41\n",
      "#healthai2026        38\n",
      "#precisionmedicine   38\n",
      "#nlp                 36\n",
      "#icmlai2026          34\n",
      "#datascience         33\n",
      "#aihealthcare        33\n",
      "#deeplearning        32\n",
      "#semanticweb         28\n",
      "#ontologies          27\n",
      "#llms                26\n",
      "#nlproc              26\n",
      "#aiconference        26\n",
      "#iswc2025            26\n",
      "#running             24\n",
      "#genai               23\n",
      "#airesearch          22\n",
      "#opensource          20\n",
      "\n",
      "==================== mastodon.ai ====================\n",
      "Top words:\n",
      "\n",
      "Top hashtags:\n",
      "\n",
      "==================== ai.community ====================\n",
      "Top words:\n",
      "\n",
      "Top hashtags:\n",
      "\n",
      "==================== social.ai ====================\n",
      "Top words:\n",
      "\n",
      "Top hashtags:\n",
      "\n",
      "==================== mstdn.art ====================\n",
      "Top words:\n",
      "\n",
      "Top hashtags:\n",
      "\n",
      "==================== creators.social ====================\n",
      "Top words:\n",
      "apple                9267\n",
      "news                 8432\n",
      "die                  5733\n",
      "und                  5218\n",
      "apfeltalk            4947\n",
      "magazin              4745\n",
      "der                  4507\n",
      "iphone               3884\n",
      "mit                  3405\n",
      "das                  3320\n",
      "für                  2958\n",
      "auf                  2838\n",
      "von                  2726\n",
      "hat                  2011\n",
      "den                  2005\n",
      "ein                  2001\n",
      "pro                  1957\n",
      "neue                 1950\n",
      "ios                  1762\n",
      "eine                 1540\n",
      "ist                  1387\n",
      "mac                  1340\n",
      "dem                  1319\n",
      "appl                 1184\n",
      "ipad                 1099\n",
      "ich                  1082\n",
      "sich                 1028\n",
      "des                  1025\n",
      "bei                  1011\n",
      "neuen                998\n",
      "\n",
      "Top hashtags:\n",
      "#news                4272\n",
      "#apple               2065\n",
      "#iphone              1297\n",
      "#tellerrand          807\n",
      "#mac                 599\n",
      "#services            554\n",
      "#ipad                416\n",
      "#appleintelligence   375\n",
      "#ki                  372\n",
      "#zubehr              347\n",
      "#feature             335\n",
      "#datenschutz         235\n",
      "#knstlicheintelligenz 224\n",
      "#tv                  213\n",
      "#appletv             210\n",
      "#appstore            205\n",
      "#technologie         199\n",
      "#watch               198\n",
      "#applewatch          193\n",
      "#vision              180\n",
      "#music               179\n",
      "#ios26               172\n",
      "#siri                171\n",
      "#timcook             170\n",
      "#twitch              169\n",
      "#twitchde            167\n",
      "#ios18               163\n",
      "#update              160\n",
      "#applevisionpro      153\n",
      "#markgurman          150\n",
      "\n",
      "==================== glitch.social ====================\n",
      "Top words:\n",
      "\n",
      "Top hashtags:\n",
      "\n",
      "==================== fediscience.org ====================\n",
      "Top words:\n",
      "2025                 192\n",
      "https                168\n",
      "org                  105\n",
      "com                  103\n",
      "utc                  101\n",
      "source               97\n",
      "eumetsat             86\n",
      "science              69\n",
      "new                  50\n",
      "earthquake           42\n",
      "social               41\n",
      "people               40\n",
      "mastodon             36\n",
      "open                 34\n",
      "research             33\n",
      "data                 32\n",
      "time                 31\n",
      "details              31\n",
      "report               30\n",
      "right                30\n",
      "university           30\n",
      "now                  29\n",
      "doi                  28\n",
      "magnitude            28\n",
      "depth                28\n",
      "first                27\n",
      "green                27\n",
      "aspx                 26\n",
      "flood                26\n",
      "climate              25\n",
      "\n",
      "Top hashtags:\n",
      "#eumetsat            86\n",
      "#science             21\n",
      "#earthquake          14\n",
      "#video               12\n",
      "#education           11\n",
      "#tksst               11\n",
      "#flood               10\n",
      "#covid               10\n",
      "#research            8\n",
      "#physics             7\n",
      "#biology             7\n",
      "#academicchatter     7\n",
      "#virtualobservatory  7\n",
      "#ai                  6\n",
      "#children            6\n",
      "#openaccess          6\n",
      "#medicine            6\n",
      "#animals             6\n",
      "#genetics            6\n",
      "#trump               5\n",
      "#uspol               5\n",
      "#uspolitics          5\n",
      "#water               5\n",
      "#history             5\n",
      "#technology          5\n",
      "#tech                5\n",
      "#wildlife            5\n",
      "#radicalright        5\n",
      "#climate             5\n",
      "#engineering         4\n",
      "\n",
      "==================== datasci.social ====================\n",
      "Top words:\n",
      "https                1746\n",
      "com                  940\n",
      "data                 856\n",
      "org                  499\n",
      "new                  467\n",
      "science              399\n",
      "social               380\n",
      "github               312\n",
      "research             285\n",
      "2025                 278\n",
      "des                  273\n",
      "les                  270\n",
      "2024                 258\n",
      "2023                 250\n",
      "here                 248\n",
      "now                  246\n",
      "paper                245\n",
      "time                 237\n",
      "rstats               224\n",
      "der                  219\n",
      "html                 216\n",
      "network              216\n",
      "transport            211\n",
      "nerds                210\n",
      "open                 209\n",
      "work                 200\n",
      "using                197\n",
      "die                  189\n",
      "pdf                  186\n",
      "twitter              185\n",
      "\n",
      "Top hashtags:\n",
      "#rstats              207\n",
      "#datascience         109\n",
      "#ai                  58\n",
      "#scotland            58\n",
      "#python              49\n",
      "#data                49\n",
      "#r                   43\n",
      "#ic2s2               42\n",
      "#dataanalysis        33\n",
      "#research            32\n",
      "#healthdatascience   31\n",
      "#copenhagen          31\n",
      "#machinelearning     30\n",
      "#phd                 28\n",
      "#mastoadmin          27\n",
      "#duckdb              27\n",
      "#networkscience      27\n",
      "#spanishoddata       26\n",
      "#gis                 26\n",
      "#bioinformatics      22\n",
      "#ucph                22\n",
      "#mastodon            21\n",
      "#dataviz             19\n",
      "#openscience         17\n",
      "#mobility            17\n",
      "#quarto              16\n",
      "#introduction        16\n",
      "#statistics          16\n",
      "#serverinfo          16\n",
      "#bigdata             16\n",
      "\n",
      "==================== scicomm.xyz ====================\n",
      "Top words:\n",
      "https                4148\n",
      "com                  3333\n",
      "2025                 2205\n",
      "org                  2112\n",
      "climate              1718\n",
      "science              1289\n",
      "new                  1277\n",
      "news                 979\n",
      "climatechange        928\n",
      "research             837\n",
      "energy               788\n",
      "article              675\n",
      "now                  673\n",
      "time                 651\n",
      "here                 642\n",
      "data                 622\n",
      "first                604\n",
      "which                566\n",
      "been                 564\n",
      "people               544\n",
      "space                515\n",
      "articles             508\n",
      "these                502\n",
      "other                482\n",
      "open                 479\n",
      "may                  471\n",
      "over                 456\n",
      "today                441\n",
      "nature               439\n",
      "html                 437\n",
      "\n",
      "Top hashtags:\n",
      "#climatechange       924\n",
      "#climate             884\n",
      "#scicomm             331\n",
      "#energy              319\n",
      "#books               212\n",
      "#astronomy           186\n",
      "#science             149\n",
      "#birds               107\n",
      "#bookstodon          107\n",
      "#openscience         105\n",
      "#bookreview          105\n",
      "#writing             97\n",
      "#blog                91\n",
      "#blogging            89\n",
      "#cdc                 75\n",
      "#uspol               69\n",
      "#environment         69\n",
      "#h5n1                68\n",
      "#mars                65\n",
      "#evolution           64\n",
      "#exoplanets          64\n",
      "#plants              64\n",
      "#ornithology         63\n",
      "#indieweb            63\n",
      "#birdflu             63\n",
      "#physics             62\n",
      "#nature              61\n",
      "#ukpol               58\n",
      "#space               56\n",
      "#astrodon            55\n",
      "\n",
      "==================== mastodon.social ====================\n",
      "Top words:\n",
      "https                4049\n",
      "com                  3277\n",
      "2025                 1965\n",
      "mastodon             940\n",
      "utm_source           894\n",
      "utm_medium           890\n",
      "news                 647\n",
      "jetpack_social       530\n",
      "now                  515\n",
      "new                  504\n",
      "die                  410\n",
      "html                 396\n",
      "que                  390\n",
      "tabirleri            330\n",
      "radio                309\n",
      "november             308\n",
      "trump                290\n",
      "dlvr                 289\n",
      "der                  286\n",
      "und                  283\n",
      "org                  282\n",
      "les                  261\n",
      "world                241\n",
      "after                232\n",
      "servis               231\n",
      "his                  228\n",
      "anlama               223\n",
      "net                  220\n",
      "gelir                220\n",
      "ruya                 220\n",
      "\n",
      "Top hashtags:\n",
      "#servis              130\n",
      "#tamir               130\n",
      "#teknikservis        130\n",
      "#istanbul            99\n",
      "#24teknikservis      97\n",
      "#adsb                78\n",
      "#nowplaying          77\n",
      "#planealert          67\n",
      "#planefence          67\n",
      "#news                65\n",
      "#tibrnowplaying      57\n",
      "#theindiebeatnowplaying 57\n",
      "#gaming              47\n",
      "#music               36\n",
      "#nsfw                35\n",
      "#radiodigitalmalayali 33\n",
      "#indianlofi          33\n",
      "#lofiradio           33\n",
      "#ai                  33\n",
      "#coskunanahtar       32\n",
      "#cilingir            31\n",
      "#kilitacma           31\n",
      "#otocilingir         31\n",
      "#kasacilingir        31\n",
      "#afropages           31\n",
      "#photography         30\n",
      "#trump               28\n",
      "#kjac                27\n",
      "#thecoloradosound    27\n",
      "#motostorie          26\n",
      "\n",
      "==================== techhub.social ====================\n",
      "Top words:\n",
      "com                  3567\n",
      "https                3232\n",
      "les                  2315\n",
      "news                 1928\n",
      "des                  1800\n",
      "actu                 1634\n",
      "presse               1531\n",
      "2025                 1514\n",
      "une                  1489\n",
      "chars                1477\n",
      "sur                  1225\n",
      "pour                 1180\n",
      "est                  1177\n",
      "dans                 1026\n",
      "france               1001\n",
      "twitter              862\n",
      "novembre             725\n",
      "avec                 691\n",
      "par                  691\n",
      "html                 658\n",
      "monde                653\n",
      "qui                  618\n",
      "plus                 612\n",
      "que                  573\n",
      "new                  558\n",
      "info                 524\n",
      "après                505\n",
      "son                  479\n",
      "gutenberg            437\n",
      "lemonde              433\n",
      "\n",
      "Top hashtags:\n",
      "#news                1528\n",
      "#actu                1475\n",
      "#presse              1475\n",
      "#photography         407\n",
      "#nature              381\n",
      "#memes               263\n",
      "#spongebob           255\n",
      "#lemonde             216\n",
      "#ai                  205\n",
      "#guide               182\n",
      "#monsterdon          172\n",
      "#meteo               137\n",
      "#20minutes           128\n",
      "#lefigaro            122\n",
      "#microsoft           107\n",
      "#c64                 99\n",
      "#franceinfo          95\n",
      "#l                   89\n",
      "#3dprinting          67\n",
      "#additivemanufacturing 65\n",
      "#evergreen           64\n",
      "#leparisien          63\n",
      "#tech                60\n",
      "#windows11           54\n",
      "#monsterdondoublefeature 54\n",
      "#liberation          48\n",
      "#street              46\n",
      "#alligator1980       44\n",
      "#footmercato         43\n",
      "#technology          43\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "instances = [\n",
    "    \"sigmoid.social\",\n",
    "    \"mastodon.ai\",\n",
    "    \"ai.community\",\n",
    "    \"social.ai\",\n",
    "    \"mstdn.art\",\n",
    "    \"creators.social\",\n",
    "    \"glitch.social\",\n",
    "    \"fediscience.org\",\n",
    "    \"datasci.social\",\n",
    "    \"scicomm.xyz\",\n",
    "    \"mastodon.social\",\n",
    "    \"techhub.social\",\n",
    "]\n",
    "\n",
    "START_DATE = datetime(2022, 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "stopwords = {\n",
    "    \"the\", \"and\", \"for\", \"that\", \"this\", \"you\", \"with\", \"are\", \"was\", \"have\",\n",
    "    \"but\", \"not\", \"from\", \"your\", \"just\", \"like\", \"they\", \"their\", \"about\",\n",
    "    \"what\", \"when\", \"where\", \"there\", \"them\", \"out\", \"get\", \"can\", \"all\",\n",
    "    \"one\", \"will\", \"into\", \"more\", \"also\", \"some\", \"has\", \"had\", \"our\",\n",
    "    \"any\", \"who\", \"why\", \"how\", \"its\", \"she\", \"him\", \"her\", \"then\", \"than\",\n",
    "    \"too\", \"very\", \"did\", \"does\", \"didn\", \"don\", \"could\", \"would\", \"should\",\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"AcademicResearchBot/1.0 (contact: your.email@example.com)\"\n",
    "}\n",
    "\n",
    "def parse_dt(dt_str):\n",
    "    if isinstance(dt_str, datetime):\n",
    "        return dt_str\n",
    "    if dt_str.endswith(\"Z\"):\n",
    "        dt_str = dt_str[:-1] + \"+00:00\"\n",
    "    return datetime.fromisoformat(dt_str)\n",
    "\n",
    "def safe_get(url, params=None, max_retries=3, timeout=30):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"  Connection error ({attempt+1}/{max_retries}): {e}\")\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print(f\"  Timeout ({attempt+1}/{max_retries}): {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Request failed: {e}\")\n",
    "            break\n",
    "        time.sleep(2 * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "def fetch_instance_statuses(instance, start_date, max_pages=300, limit=40, sleep=3.0):\n",
    "    base = f\"https://{instance}/api/v1/timelines/public\"\n",
    "    max_id = None\n",
    "    statuses = []\n",
    "\n",
    "    for _ in range(max_pages):\n",
    "        params = {\"local\": \"true\", \"limit\": limit}\n",
    "        if max_id is not None:\n",
    "            params[\"max_id\"] = max_id\n",
    "\n",
    "        r = safe_get(base, params=params)\n",
    "        if r is None:\n",
    "            print(\"  Giving up on this instance for now.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            batch = r.json()\n",
    "        except ValueError:\n",
    "            print(\"  Failed to parse JSON, stopping.\")\n",
    "            break\n",
    "\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        stop = False\n",
    "        for s in batch:\n",
    "            created_at = parse_dt(s[\"created_at\"])\n",
    "            if created_at < start_date:\n",
    "                stop = True\n",
    "                break\n",
    "            statuses.append(s)\n",
    "\n",
    "        max_id = min(int(s[\"id\"]) for s in batch)\n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    return statuses\n",
    "\n",
    "def extract_words(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
    "    return re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "word_freqs = {}\n",
    "hashtag_freqs = {}\n",
    "\n",
    "for inst in instances:\n",
    "    print(f\"\\nFetching from {inst}...\")\n",
    "    statuses = fetch_instance_statuses(inst, START_DATE, max_pages=200, limit=40, sleep=1.0)\n",
    "    print(f\"  Got {len(statuses)} statuses\")\n",
    "\n",
    "    wc = Counter()\n",
    "    hc = Counter()\n",
    "\n",
    "    for s in statuses:\n",
    "        html = s.get(\"content\", \"\")\n",
    "        text = BeautifulSoup(html, \"html.parser\").get_text(\" \")\n",
    "        words = [\n",
    "            w for w in extract_words(text)\n",
    "            if w not in stopwords and len(w) > 2\n",
    "        ]\n",
    "        wc.update(words)\n",
    "\n",
    "        tags = s.get(\"tags\", [])\n",
    "        hc.update(t[\"name\"].lower() for t in tags if \"name\" in t)\n",
    "\n",
    "    word_freqs[inst] = wc\n",
    "    hashtag_freqs[inst] = hc\n",
    "\n",
    "TOP_N = 30\n",
    "\n",
    "for inst in instances:\n",
    "    print(f\"\\n==================== {inst} ====================\")\n",
    "    print(\"Top words:\")\n",
    "    for word, count in word_freqs[inst].most_common(TOP_N):\n",
    "        print(f\"{word:20} {count}\")\n",
    "\n",
    "    print(\"\\nTop hashtags:\")\n",
    "    for tag, count in hashtag_freqs[inst].most_common(TOP_N):\n",
    "        print(f\"#{tag:19} {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
